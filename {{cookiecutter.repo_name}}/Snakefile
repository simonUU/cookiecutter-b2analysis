# Example workflow with snakemake to process data and combine files
#
# Author:  Simon (simon.wehle@desy.de)
#
#

#
# Define your datasets here
#
DATASETS = { 
	'sig' : 'data/example_mc/*.root',
#	'uubar' : ['data/mc12/*_uubar.root', 2] # Always process 2 files per job
#	'ddbar' : 'data/mc12/*_ddbar.root',
#	'mixed' : 'data/mc12/*_mixed.root',
#	'charged' : 'data/mc12/*_charged.root',
# 	'data' : [['/path/to/data/GoodRuns/r*','/all/mdst/sub00/*.root'] , 100], # for each first glob, put 100 files per job

}

#
# Output location
#
OUTPUT = 'data/ntuples'

#
# Version name for the processed files
#
VERSION = '{{cookiecutter.analysis_version}}'
STEERINGFILE = 'reconstruction/{{cookiecutter.repo_name}}_{{cookiecutter.analysis_version}}.py'

#
# Helper functions
# 
include: "workflow/helpers.smk"
datasets = GlobFilesToIndex(DATASETS, glob_storage=None)
# >>> Helper END


# Basf2 processing
#
rule process_basf2:
	""" Run the reconstruction scripts, change the python executeble """
	input: lambda wildcards: datasets.get_files_by_index(wildcards.cat, int(wildcards.index))
	output: OUTPUT+'/'+VERSION+'_{index}_{cat}.root'
	run:
		infiles = " -i ".join(i for i in input) 
		shell("basf2 " + STEERINGFILE + " -i " +infiles + " -o {output} ")

rule combine_files_hadd:	
	""" Combine multiple processed files, example agg.py file used  """
	input: lambda wildcards: expand(OUTPUT+'/'+VERSION+'_{ifile}_{cat}.root', cat=wildcards.cat, ifile=datasets.range_nfiles(wildcards.cat))
	output: OUTPUT+'/'+VERSION+'_combined_{cat}.root'
	shell: 	"hadd {output} {input}"

rule basf2:
	""" Define all final datasets """
	input: 	expand(OUTPUT+'/'+VERSION+'_combined_{cat}.root', cat=list(DATASETS.keys()))

